{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quarterly-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import  cpu_count, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mysterious-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    def __init__(self,Y_M, Y_N = None, Z_N = None, sigma_e_init = 0, W_init = 0 ):\n",
    "        self.Y_N = Y_N\n",
    "        self.Y_M = Y_M\n",
    "        self.Z_N = Z_N\n",
    "        self.Z_M = None\n",
    "        \n",
    "        \n",
    "        self.M = len(Y_M)\n",
    "        self.N = len(Y_N)  if type(self.Y_N) != type(None) else 0\n",
    "        self.S = self.M + self.N\n",
    "        \n",
    "        self.sigma_e = sigma_e_init\n",
    "        self.W = W_init\n",
    "        \n",
    "        self.EZ = None\n",
    "        self.EZZ = None\n",
    "        \n",
    "        \n",
    "    def e_step(self):\n",
    "        A = (self.W**2) * (self.sigma_e**-2) + 1\n",
    "        B = self.W*self.Y_M*(self.sigma_e**-2)\n",
    "        \n",
    "        self.EZ = B/A\n",
    "        self.EZZ = (1/A)  + (B/A)**2\n",
    "        \n",
    "    def m_step(self):\n",
    "        # Must update W first, as it is required for sigma\n",
    "        self.W = self.new_W()\n",
    "        self.sigma_e = self.new_sigma_e()\n",
    "        \n",
    "        \n",
    "    def new_sigma_e(self):\n",
    "        _1 = np.sum((self.Y_N-W*self.Z_N)**2) if type(self.Y_N) != type(None) else 0\n",
    "        _2 = np.sum(self.Y_M**2)\n",
    "        _3 = -2*self.W*np.sum(self.Y_M*self.EZ)\n",
    "        _4 = self.W**2 * np.sum(self.EZZ)\n",
    "\n",
    "        return np.sqrt((_1 + _2 + _3 + _4)/self.S)\n",
    "    \n",
    "    \n",
    "    def new_W(self):\n",
    "        _1 = np.sum(self.Y_N*self.Z_N)  if type(self.Y_N) != type(None) else 0\n",
    "        _2 = np.sum(self.Y_M*self.EZ)\n",
    "        _3 = np.sum(self.Z_N**2) if type(self.Y_N) != type(None) else 0\n",
    "        _4 = np.sum(self.EZZ)\n",
    "        return (_1 + _2)/(_3 + _4)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "therapeutic-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_selector(arr, bins):\n",
    "    counts, bins, bars = plt.hist(arr,bins=bins)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    idx = np.argmax(counts)\n",
    "    return np.mean([bins[idx], bins[idx+1]])\n",
    "\n",
    "def param_selection(sigma_e_arr, W_arr, bins = 50):\n",
    "    return peak_selector(sigma_e_arr, bins), peak_selector(W_arr, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minute-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM(data, reps = 1000, max_iter_ = 10000):\n",
    "    # Max_iter is for each rep\n",
    "    ## ALL data missing\n",
    "    # Since data has been standardised, noise values should be expected to be small. Hence we initialise as follows:\n",
    "\n",
    "    sigma_e_arr = []\n",
    "    w_arr = []\n",
    "\n",
    "\n",
    "    for i in range(reps):\n",
    "        sigma_init = np.random.uniform(0,0.1)\n",
    "        W_init = np.random.uniform(-10,10)\n",
    "\n",
    "        EM1 = EM(data, sigma_e_init = sigma_init, W_init = W_init)\n",
    "\n",
    "        prev_sigma_e = EM1.sigma_e\n",
    "        prev_W = EM1.W\n",
    "        not_converged = True\n",
    "        i = 0\n",
    "\n",
    "        while not_converged and i < max_iter_:\n",
    "            EM1.e_step()\n",
    "            EM1.m_step()\n",
    "            i += 1\n",
    "\n",
    "            # If a paramter value becomes nan, we set the converged value to the previous known real number\n",
    "            if np.isnan(EM1.sigma_e) or np.isnan(EM1.W):\n",
    "                EM1.sigma_e = prev_sigma_e\n",
    "                EM1.W = prev_W\n",
    "                not_converged = False\n",
    "            \n",
    "            # To avoid divide by 0\n",
    "            if EM1.sigma_e == 0:\n",
    "                EM1.sigma_e += 0.0000000001\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(prev_sigma_e - EM1.sigma_e) < 0.0001 and abs(prev_W - EM1.W) < 0.0001:\n",
    "                not_converged = False\n",
    "            else:\n",
    "                prev_sigma_e = EM1.sigma_e\n",
    "                prev_W = EM1.W\n",
    "            \n",
    "        \n",
    "\n",
    "        sigma_e_arr.append(EM1.sigma_e)\n",
    "        w_arr.append(EM1.W)\n",
    "\n",
    "    # Selection method selects most common param value however, the assumption that this is equivalent to the value of highest ll is untrue.\n",
    "    selected_sigma_e, selected_w = param_selection(sigma_e_arr, w_arr)\n",
    "\n",
    "    EM2 = EM(data, sigma_e_init = selected_sigma_e, W_init = selected_w)\n",
    "    EM2.e_step()\n",
    "\n",
    "    return selected_w * EM2.EZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "southern-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets\n",
    "data = pd.read_csv(\"../../Data/X_train_0_WELL.csv\", sep=',')\n",
    "data_copy = pd.read_csv(\"../../Data/X_train_0_WELL.csv\", sep=',')\n",
    "X_test = pd.read_csv(\"../../Data/X_test_0_WELL.csv\", sep=',')\n",
    "X_test_copy = pd.read_csv(\"../../Data/X_test_0_WELL.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "careful-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ['SP', 'DTS', 'DTC', 'NPHI', 'PEF', 'GR', 'RHOB', 'CALI', 'DCAL', 'SGR']\n",
    "Numerical = ['CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'SGR', 'NPHI', 'PEF','DTC', 'SP', 'BS', 'ROP', 'DTS', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC','ROPA', 'RXO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frank-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get expected values after removing noise for training set and assign to dataframe\n",
    "def replace_data(well):\n",
    "    return_list = []\n",
    "    for feature in Numerical:\n",
    "        return_list.append(run_EM(data[data['WELL'] == well][feature].to_numpy()))\n",
    "    return return_list\n",
    "        \n",
    "# Get expected values after removing noise for testing set and assign to dataframe\n",
    "def replace_test(well):\n",
    "    return_list = []\n",
    "    for feature in Numerical:\n",
    "         return_list.append(run_EM(X_test[X_test['WELL'] == well][feature].to_numpy()))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=48) as pool:\n",
    "    rl = pool.map(replace_data, range(1,99))\n",
    "\n",
    "for well in range(1,99):\n",
    "    for feature in range(len(Numerical)):\n",
    "        data.loc[data['WELL'] == well, Numerical[feature]] = rl[well-1][feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data == data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-notification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Push to CSV\n",
    "data.to_csv(\"../../Data/Noise_free/X_train_noise_free_linear_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=48) as pool:\n",
    "    rl2 = pool.map(replace_test, range(99,109))\n",
    "\n",
    "for well in range(99,109):\n",
    "    for feature in range(len(Numerical)):\n",
    "        X_test.loc[X_test['WELL'] == well, Numerical[feature]] = rl2[well-99][feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test == X_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to CSV\n",
    "X_test.to_csv(\"../../Data/Noise_free/X_test_noise_free_linear_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-republican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-article",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-indian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-juice",
   "metadata": {},
   "source": [
    "* **** Still need to optimise param selection method\n",
    "\n",
    "* create new dataframe with f(Z) - done\n",
    "* Retrain NN with orignial dataset\n",
    "* train NN with new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
